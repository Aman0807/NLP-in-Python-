{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, our aim is to get some insights about the dataset which we cleaned in the previous section. \n",
    "\n",
    "We will find the most common words and represent it using *matplotlib* and *wordclouds* modules to get an idea about the topics around which the conversation revolves in the show. Later on, we can do the same for TBBT and compare the difference in languages, if any. \n",
    "\n",
    "[Matplotlib Documentation](https://matplotlib.org/)<br>\n",
    "[Word Cloud Documentation](https://amueller.github.io/word_cloud/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_t = data_dtm.T\n",
    "data_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Top 10 words in each episode\n",
    "\n",
    "\n",
    "top_dict = {}\n",
    "for c in data_t.columns:\n",
    "    top = data_t[c].sort_values(ascending=False).head(10)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a lot of top words are those words which are generally used in day-to-day conversation. These words will not be helpful in finding meaningful insights. So I have got rid of them using the text module of scikit-learn. There is already a list of such words called English_Stop_Words. To this list, we'd be adding the most common words found above. \n",
    "\n",
    "Further, we would also be getting rid of profanity since it's abundent in our transcripts and don't help in our analysis. \n",
    "To do so, I have downloaded a file containing bad_words made available by folks at CMU. [This is the list](https://www.cs.cmu.edu/~biglou/resources/)   Not for the faint of heart, though !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for item in stop_words:\n",
    "    words.append(item)\n",
    "    \n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To above list of 318 words add the most common words.  \n",
    "for episode in episodes:\n",
    "    for t in top_dict[episode]:\n",
    "        words.append(t[0])\n",
    "        \n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading the bad-words file\n",
    "\n",
    "bad_words = pd.read_csv(r'E:\\Silicon Valley\\bad-words.txt')\n",
    "type(bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in bad_words.abbo:\n",
    "    words.append(item)\n",
    "    \n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we have 1780 words  which need to be removed from our DTM \n",
    "# Recreate document-term matrix\n",
    "\n",
    "cv = CountVectorizer(stop_words=words)\n",
    "data_cv = cv.fit_transform(data.transcripts)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = data.index\n",
    "data_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing dependancies and creating wordclouds for each episode\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "fig = plt.figure(figsize = (15,5) )\n",
    "\n",
    "for index , episode in enumerate(episodes):\n",
    "    wordcloud = WordCloud(stopwords = words,max_font_size=50, background_color='white').generate(data.transcripts[episode])\n",
    "    \n",
    "    plt.subplot(2, 4, index+1)\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    plt.title(episode)\n",
    "    \n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some of the themes  areound which the  conversation revolves are - company, money, world, obsessed, vision, big etc. \n",
    "These words might be used by ambitious people trying to build something. Especially, in the tech industry. This is one of the reasons why I have been attracted to the show as I personally like to talk about these things too.  \n",
    "\n",
    "\n",
    "Next up, lets's conduct a Sentiment Analysis on the tweets about the show and see what we can find. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform Sentiment Analysis we will be using tweepy module to collect tweets and textblob to perform analysis. \n",
    "\n",
    "[Tweepy Documentation](https://tweepy.readthedocs.io/en/latest/)<br>\n",
    "[TextBlob Documentation](https://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "\n",
    "access_token = '1133364060718678017-NlAtZqnmW1XQrHiFRFaSn2pkgg9bjN'\n",
    "access_token_secret = 'nwRvtguHBdlAYESq417sIbOgUiiCioHWrwye2vLCIGcBZ'\n",
    "consumer_key = 'Mup6xb3v5b3mL85kmESyU4BDC'\n",
    "consumer_secret = 'nr1zsiK4Mazq8H677CPUFi6C0sal4LFdtIyqDnOLCNxIbUkI0T'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Searching tweets containing 'Silicon Valley HBO'\n",
    "\n",
    "public_tweets = api.search('Silicon Valley HBO', lang = 'en' , count = 100)\n",
    "print(public_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting the polarity and subjectivity for each tweet\n",
    "\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    print(analysis.sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating a dataframe of tweets\n",
    "\n",
    "tweet_data = pd.DataFrame(data=[tweet.text for tweet in public_tweets], columns=['Tweets'])\n",
    "\n",
    "display(tweet_data.sample(10))\n",
    "tweet_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One another approach I have used is to use **Vader Sentiment Analysis** which gives us the polarity in terms terms positive, negative , neutral and a overall compound sentiment. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "list = []\n",
    "\n",
    "for index, row in tweet_data.iterrows():\n",
    "    score = analyser.polarity_scores(row['Tweets'])\n",
    "    list.append(score)\n",
    "    \n",
    "list_series = pd.Series(list)\n",
    "tweet_data['polarity'] = list_series.values\n",
    "\n",
    "display(tweet_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is barely any negative sentiment around the show and most tweets are either neutral or positive. This might explains why the show has been very popular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Saving the above  dataframe for future use\n",
    "tweet_data.to_csv('E:\\Silicon Valley\\Silicon_Valley_VaderSentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now, I have tried to answer the first two of the three question I wanted to answer. For the last bit I will be repeating the same steps for TBBT and making comparisons. It will be carried out on a seperate notebook. \n",
    "\n",
    "\n",
    "Cheers, Always Blue!Always Blue!Always Blue!Always Blue! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
